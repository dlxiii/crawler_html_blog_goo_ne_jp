{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals\n",
    "import logging\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import math\n",
    "\n",
    "try:\n",
    "    from urllib.parse import urlparse  # py3\n",
    "except:\n",
    "    from urlparse import urlparse  # py2\n",
    "\n",
    "import pdfkit\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "\n",
    "html_template = \"\"\"\n",
    "<!DOCTYPE html>\n",
    "<html lang=\"ja\">\n",
    "<head>\n",
    "    <meta charset=\"UTF-8\">\n",
    "</head>\n",
    "<body>\n",
    "{content}\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "url = \"https://blog.goo.ne.jp/0424725533\"\n",
    "\n",
    "\n",
    "def getList(url):\n",
    "    postNum = 0\n",
    "    hrefList = []\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "    div = soup.find('div',id=\"mod-categories\",class_='module')\n",
    "    list = div.find_all('span',class_='mod-cat-count')\n",
    "    for span in list:\n",
    "        num = float(int(span.string.replace('(', '').replace(')', '')))\n",
    "        postNum = postNum + num\n",
    "        pageNum = math.ceil(postNum/20)\n",
    "    for i in range(1,pageNum + 1):\n",
    "        href = 'https://blog.goo.ne.jp/0424725533/arcv/?page=' + str(i) + '&c=&st=1'\n",
    "        hrefList.append(href)\n",
    "    print('获得'+ str(len(hrefList)) +'页列表链接。')\n",
    "    return hrefList\n",
    "\n",
    "\n",
    "def getLink(href):\n",
    "    htmlList = []\n",
    "    post = 0\n",
    "    postNum = 0\n",
    "    print('解析：' + href + '列表中......')\n",
    "    response = requests.get(href)\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "    div = soup.find('div',class_='entry-body-text')\n",
    "    for li in div.find_all('li'): \n",
    "        title = li.find('span').get_text()\n",
    "        contains = title.find('東大大学院')\n",
    "        if contains >= 0:\n",
    "            html = li.a.get('href')\n",
    "            htmlList.append(html)\n",
    "    print('>> 获取' + str(len(htmlList)) + '篇文章')\n",
    "    return htmlList\n",
    "\n",
    "\n",
    "def getPage(html):\n",
    "    response = requests.get(html)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    body = soup.find('div',class_=\"entry\")\n",
    "    top = body.find('div',class_=\"entry-top\")\n",
    "    title = top.find('h3').get_text()\n",
    "    time = top.find(class_='entry-top-info-time').get_text()\n",
    "    cen = body.find('div',class_=\"entry-body\")\n",
    "    context = cen.find('div',class_=\"entry-body-text\")\n",
    "    title_loc = soup.new_tag(\"center\")\n",
    "    title_tag = soup.new_tag('h1')\n",
    "    time_loc = soup.new_tag(\"center\")\n",
    "    title_tag.string = title\n",
    "    title_loc.insert(1, title_tag)\n",
    "    context.insert(0, title_loc)\n",
    "    time_loc.insert(1,time)\n",
    "    context.insert(1, time_loc)\n",
    "    page = str(context)\n",
    "    page = html_template.format(content=page).encode(\"utf-8\")\n",
    "    with open(title + \".html\", 'wb') as f:\n",
    "        f.write(page)\n",
    "        print('>> 抓取文章，命名为：' + title)\n",
    "    return title\n",
    "\n",
    "\n",
    "def savePdf(file,name):\n",
    "    options = {\n",
    "    'page-size': 'B5',\n",
    "    'margin-top': '15mm',\n",
    "    'margin-right': '15mm',\n",
    "    'margin-bottom': '15mm',\n",
    "    'margin-left': '15mm',\n",
    "    'encoding': \"UTF-8\",\n",
    "    'custom-header': [('Accept-Encoding', 'gzip')],\n",
    "    'cookie': [\n",
    "            ('cookie-name1', 'cookie-value1'),\n",
    "            ('cookie-name2', 'cookie-value2'),],\n",
    "    'minimum-font-size': '40',}\n",
    "    pdfkit.from_file(file,name,options=options)\n",
    "    \n",
    "    \n",
    "if __name__==\"__main__\":\n",
    "    hrefList = getList(url)\n",
    "    start = time.time()\n",
    "    for href in hrefList:\n",
    "        htmlList = getLink(href)\n",
    "        for html in htmlList:\n",
    "            title = getPage(html)\n",
    "            file_path = './' + str(title) + '.html'\n",
    "            file_name = str(title) + '.pdf'\n",
    "            savePdf(file_path,file_name)            \n",
    "    total_time = time.time() - start\n",
    "    print(u\"总共耗时：%f 秒\" % total_time)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
